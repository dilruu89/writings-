\documentclass[a4paper]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Litrature Review}
\author{Samudra Herath}

\begin{document}
\maketitle

\section*{Simulating the Record Linkage Problem}
This paper describes a simulation tool that is designed for record linkage problem. Most of the simulation related work has been done in the early stages of record linkage process mainly in quality of pairwise matching and as a prerequisites, early steps of preprocessing and blocking.Most of the simulation tools focused on simulating the realistic records including the realistic errors. The simulation work discussed in this paper is designed based on Bonini's Paradox, which says the that simulation of a more complex system becomes more complete and the more complex it becomes more harder to understand. Thus they have concentrate on simplifying the early stages of record linkage in the process of probabilistic record linkage steps and threshold selection.By simplifying the model they have gained simple results and a universal simulation framework that could be applied for any application, any region or any set of comparison algorithms.The main high level requirements for this simulation has required flexibility, scale,control, simplicity, repeatability, ensemble creation, ground truth and modularity.For the consistent meaning of the fields, numeric value is assigned for all of the identifier fields.
Through this exact matches can be identified. easily and they assumes that the numerical close values represent values that would be close after comparison which is useful in identifying exact,near matches vs complete mismatches.

This designed start by generating a original records for a set of entities which is called '\textbf{\textit{census}}'.Next step is creating set of DBs by sampling, adding errors and removing missing data. Two parameters per field is allowed to represent the fields as a numeric value.Those are the distribution and the parameters of the distribution. Three models has used for DB sampling namely without replacement, with replacement and with replacement and poison repetition.Later a given subset has chosen for error adding, dropping data for the records in the DBs. A missing data indicated by making the field value to zero. Then the record can be compared returning a comparison outcome matrix, rows correspond to the pairwise comparisons and columns correspond to the fields.The implementation has done using Julia which is free, open-source and a high level language with a own byte compiler.

An example work has been test using Max-Min transitive closure which requires some details of individual field matches.The results has indicated use of network-wide data improves the detection probability unilaterally. When it comes to the limitations of this work first comes the simulation of real errors. Describe a model for real errors is a problem in all linkage simulation toolkits. The other drawback is that this simulation only considers uniform distribution for probabilistic linkage matching model.Future works suggests that more incorporating structured errors and use of simulations in more advanced techniques for record linkage. 

\section*{Privacy-preserving Data Linkage Protocols} 
In this paper \cite{Keefe2004}  the authors presents several privacy-preserving data linkage and data extraction protocols.Many of the existing linkage projects use a highly trusted third party linkage unit for deterministic or probabilistic linkage.The third party linkage unit generates project identifiers for the linked records and supply the linked information (ex: clinical information in health records) to the researcher or user. But there are cases that some organizations are strictly prohibited to release personal identifiers to even a trusted third party where there is a need for linkage protocols which do not involve in releasing any identifier information.There are some other protocols which involves two parties only but still where additional information being revealed to those protocol parties. This paper address the question of enabling data linkage across separate data tables without requiring any additional information to be revealed to any party outside the data source. They have also address the problem of extracting a cohort of individuals data from a data source without revealing the membership of any individuals in that cohort data set.

The proposed privacy preserving linkage protocol have three parties A and B (data sources)
and a third T which is the querying party.
All operations take place on a agreed common safe prime p=2q+1 and a generator g of the subgroup QRp by the data sources. If the linkage perform on K attributes of both A and B,the protocol allows T to get the intersection size of|KA  K| and |KA| , |KB| where A and B only knows |KB| and |KA|.No data-source is learn the intersection size in this protocol.

The proposed privacy preserve data extraction protocol enables T to extract desired extra information on the linkage data set. They have used \textit{A cohort-Obscuring Protocol with minimal information leakage} which follows the property of repeated 1-out-of-N Oblivious Transfer. This has enables T to obtain extra information on linkage without revealing the cohort chosen to either A or B. 

The authors also suggest variations to their proposed protocols, which enables data sources to generate common pseudonyms without revealing any identifying information to any parties. Another variation is to show how the protocols can be generalized for any number of data sources (many database tables). The paper also present some existing protocols which have involved in trusted third partied and two party protocols. When it comes to the discussion this protocols only require encrypted hashed identifiers to be revealed to the third party and does not require any parties to share a secret key.Since the privacy preserving and data extraction protocols do not rely on a third party trust and vulnerability to collusion has minimized.

The limitation are that this protocols only link database entries with exact matching attributes. Since typographical errors are more common in real world scenarios it is future challenge to link entries with typographical errors in identifying attributes which may ultimately consider good enough match on attributes as well.

\section*{Privacy-Preserving Sequential Pattern Release}

This paper \cite{Jin2007} investigate situations where releasing frequent sequential patterns can compromise individual's privacy. They have proposed two objectives for privacy preserving: \textbf{\textit{k-anonymity}} and \textbf{\textit{$\alpha$-dissociation}} respectively. This work covers sensitive \textit{attribute values disclosure} and \textit{identification disclosure}to study the sequential patterns represent threat to privacy, which had a low attention in data mining researches.The newly proposed privacy preserving objectives can act as standard of releasing frequent sequential patterns without compromising the privacy.\textit{k-anonymity sequential patterns} from which one impossibly infers the existence of patterns with very low support;\textit{$\alpha$-dissociative sequential patterns} from which one impossibly infers an attribute value with very high certainty. 

If \textbf{P} is a sequential pattern in \textbf{D} sequence database, \textbf{support} of the sequential pattern \textbf{P} in \textbf{D} is defined as the number of sequences that contain \textbf{P}. A privacy inference channel is a subset of frequent sequential patterns from which it is probable to infer sensitive information such as \textit{non-k-anonymity} and \textit{non-$\alpha$-dissociation} patterns. The proposing privacy-preserving released frequent patterns are obtained by adjusting their support values to remove privacy inference channels while in the same time trying to maintain the database compatibility. The authors have implemented a \textbf{PICS(Privacy Inference Channel Sanitisation)} algorithm that removes all privacy inference channels according to the proposed two objectives. Incrementing small values of the support value the algorithm can sanities frequent sequential patterns for privacy preserving results, as substantiated by series of experimental results in an efficient and effective way

\section*{Privacy-preserving record linkage using Bloom filters}

This paper \cite{Schnell2009} present a new protocol for privacy preserving record linkage based on Bloom filters on q-grams of identifiers allowing errors in identifiers. Encrypting the identifiers with a standard cryptographic procedure is a obvious solution in privacy preserving record linkage. Most of the exact matching protocols do not tolerate any errors in identifiers where in the nature of of cryptographic functions, a slightest input variation may lead to large changes to the output. Probabilistic linkage doesn't require exact matching in identifiers and using string similarity functions this can improve the quality of the linkage. The aim of this paper is to introduce a new method for calculating the similarity between two strings in order to use in probabilistic record linkage.

Based on the literature there are number of protocols in this context which involves \textit{two parties, three parties} and \textit{multi parties.} Some protocols rely on phonetic transformed identifiers which follows phonetic rules and subsequently followed with one-way hash function. When compared with string comparisons, phonetic encoding tend to give more false positive links.

In this method Bloom filters were used to calculate the similarity of two encrypted strings. A bloom filter is a data structure used for checking set membership in an efficient way and also used to determine whether two sets approximately match or not. Dice coefficient has used to compute the bi-gram similarity between two strings (ex: similarity of two surnames) and bloom filter is used to encrypt the bi-grams of the two strings. The authors have achieved this by storing the q-grams of each name in separate bit array using k multiple cryptographic mappings appropriately. Then the bloom filters are compared bit by bit to calculate the similarity coefficient.The double hashing scheme has used when storing the q-grams in the bloom filters. Therefore only two independent hash functions(SHA1 ,MD5) needed to implement a Bloom filter with k hash functions without a increase in false positive probability.

q-gram similarity between unencrypted strings method was compared with the proposed method using simulated and actual databases. Results shows that the bloom filter method comparable to the performance of the unencrypted trigrams and  superior to phonetic encoding method. In actual implementation SHA1 and MD5 has replaced by a keyed hash message authentication code (HMAC) with a secret key -K. This method requires semi-trusted third party where the Bloom filters are prone to dictionary attacks. Since the current method just match string attributes only it can be extended to comparison of numerical attributes in common.Frequent attacks, re-identification of individuals from merged files and transferring the merged file to one of the data holders are some problems in this method.   



\section*{FEDERAL A Framework for Distance Aware-Privacy Preserving Record Linkage}
This paper \cite{Karapiperis2017} presents a novel record linkage framework that implements for anonymizing both string and numerical data values in cooperating distance aware encoding methods. It guarantees to provide accuracy and privacy under various models where original values are embedded into an anonymization space. They have used \textit{Low Cost Bloom Filters} and \textit{Bit Vectors } in order to anonymize string and numerical values. Using binary Hamming Space to embedded the real values they have gained low communication cost for performing distance computations, reduction of storage requirements, low communication overhead and direct application of Locality Sensitive Hashing blocking. LCBF uses Bloom filters whose size specified according to a tolerable degree of perturbation between similar strings. BVs hashes numerical values using computationally cheap operations compared to expensive cryptographic hash functions. Hamming Locality Sensitive hash technique has employed with bloom filters in the  \textit{blocking} step which identified similar bloom filters with high probability.

The Trusted Third part (TTP) follows the Honest-but-curious model to identify the anonymized records belong to the same real world entity. Then they embedded strings to H using bloom filters and argues that the number of differing q-grams between a pair of strings affect their Hamming distance. When the size of the generating bit vector becomes large in data-intensive settings which increases the computational resources. Thus the proposing method exploit hashing to shrink the dimensionality of H, to reduce the size of the bit vector while maintaining the accuracy guarantees to highest extent. The authors have used three real world data sets and they have compared the work with two state-of-two-art methods,Euclidean Vectors and standard Bloom Filters. Since most of these methods lacks of accuracy the proposed FEDERAL method has outperform compared to other existing methods.


\section*{Development and User Experience of an Open Source Data Cleaning, De-duplication and Record linkage System}

This paper \cite{Christen2009} presents an overview of \textit{Febrl} (Free Extensible Biomedical Record Linkage) system and user experiences. \textit{Febrl} is tool that includes collection of functions that requires in data cleaning, de-duplication and record linkage. This tool is known to be the only free tool available for data cleaning, de-duplication and record linkage with a Graphical User Interface (GUI). Standardization of surnames and addresses based on a probabilistic Hidden Markov Model(HMM) while for simple names a rule-based approach is used.\textit{Febrl} has varying number of techniques for indexing, comparison, Encoding and classification methods which allows users to choose any as appropriate. It is Well documented, highly configurable and extensible while having the disadvantages of poor scalability, installation complexity, need of large memory for large data, slowness, unclear error messages and requirement of Python skills in installation process are some of the main disadvantages. The authors are looking forward to provide access to SQL and ODBC databases to load input data as a future enhancement for \textit{Febrl}. 
    

\section*{Resilient Identity Crime Detection}

\cite{Phua2012ResilientDetection}	Credit application fraud is one application in identity crime which could be committed on mix of synthetic and real identity data. Fraudsters in identity crime are well organized, highly experienced and sophisticated. This paper argues that each successful credit application fraud pattern is represented by a sudden and sharp spike in duplicates within a short time, relative to the established baseline level. The two main challenges in detection systems based on data-mining layers of defense are \textbf{adaptivity} and \textbf{use of quality data}. The existing defenses made upon business rules, scorecards and fraud matching. Use of known errors in fraud matching may take long time delays and it requires manual recording of frauds. The proposing method achieves resilience by adding two real-time data mining base layers to complement the two existing non data mining layers. The new layers will not require human involvement, do not use external databases, are unsupervised algorithms and will improve detection in fraudulent applications.

The first layer Communal Detection (CD): is a whitelist-oriented approach on a fixed set of attributes. Communal relationships are nearer duplicates which reflects the social relationships from tight familial bonds to casual acquaintances.The second layer is Spike Detection(SD) : the attribute oriented approach on a variable-size set of attributes. SD find spikes to increase the suspicion score and is probe-resistance for attributes. SD strengthens CD by providing attribute weights reflect the degree of importance in attributes. SD trades off effectiveness for efficiency while CD trades off efficiency for effectiveness. Name , DOB, Gender and address are considered as most important identity attributes in most of the studies. All the numerical attributes have been treated as strings in this study and in identity crime detection data were limited to exact matches of their encrypted attributes since the encryption methods  were not revealed to the authors. The results have shown that the CD algorithm strengthened by SD algorithm's attributes weights, and with right parameter settings deliver superior and faster results than classifier experiments (using Naive Bayes, Decision trees, Logistic regression, SVM and Hoeffding decision tree). 

\section*{Probabilistic Record Linkage}
This article \cite{Sayers2016ProbabilisticLinkage} describes probabilistic record linkage through a simple exemplar with the data structures that is being used for probabilistic record linkage.The authors describes the process of calculating and interpreting matching weights and converting them into posterior probabilities of a match using \textbf{Bayes theorem}.They have considered a scenario where linking data from two files, 'master file' and a 'file of interest'.In deterministic record linkage the records will be categorized as a \textit{'match'} or \textit{'unmatch'} based on first name and last name as keys. The results has shown that according to capitalization, special characters, nicknames, alternative spellings and spelling mistakes has made the record linkage unsuccessful. But the results of joining the two files and calculating agreement patterns between the linking keys has indicated that there may be more commonality between the two files than previously identified by deterministic record linkage. Using Edit Distance to calculate partial agreements, using phonetic algorithms and q-gram approach to measure the similarity between strings more links can be found between records when it comes to probabilistic record linkage.  

Data cleaning process has used in the process in order to prevent typographical errors such as changing the case, removing punctuation, identifying nicknames and etc. In the statistical framework underlying probabilistic record linkage used in early 1950. The dissimilarity of two strings were represented as the ratio of two conditional probabilities where the two records have same agreement patterns. Probabilities of matching and mismatching usually refer as \textit{m-probability} and \textit{u-probability} respectively. \textit{m-probability}  can be seen as an indicator of data quality while \textit{u-probability} refers as chance agreement between two records which are truly unmatched.

When the number of records increase in a file resulting linkage file would have large number of potential links. Blocking or partitioning the data set can reduce the comparison space assuming that the individuals not in the block will not be a match. There exists methods which could be used to quantify the rate of linkage errors such as, comparing with gold standard, sensitivity analysis, comparing linked and unlinked data and identification of implausible matches. Thus, the authors describe a simple way of robust probabilistic record linkage using standard statistical software, and illustrates that sensitivity of results can be easily explored using different matching assumptions.

\newpage

\section*{Analysis of a Probabilistic Record Linkage Technique without human Review}
This paper \cite{Grannis2003} describes of a deterministic record linkage using \textit{Expectation Maximization Algorithm (EM)} to employ an estimator function to avoid human intervention in review process.The EM algorithm established a single true-link threshold where it estimates linkage parameters with acceptable accuracy showing improvements in deterministic algorithm. The authors have also described the performance of a probabilistic linkage algorithm implemented without human intervention which was compared with deterministic method. EM algorithm has known to be a widely use probabilistic algorithm for estimating maximum likelihood estimates of unknown parameters. The results has shown that deterministic sensitivity may decrease in data with different characteristic such as different ethnic names. They have also observed that the probabilistic method automatically adapts to the specific data while deterministic method does not.

\section*{Privacy Preserving Probabilistic Record Linkage -D E Clark }
This paper presents a new method for classifying pairs into matches and non matches using EM algorithm under the Multinomial distribution. String comparator/similarity scores has included in directly into the models for estimating matching parameters. They have also propose a new method provides protection against attack scenarios on string annonimization.

\section*{A Comparison of String Metrics for Matching Names and Records}
This paper \cite{Cohen2003ARecords} tries to formulate entity matching as a classification problem with the basic goal of classifying pairs into matches or non-matches. The authors have also described a newly implemented Java toolkit for conduct name matching which includes various no of techniques of name matching methods.First half of the work is on string matching using various distance metrics including distance functions, edit distance metrics, string comparators, token based methods and hybrid methods.\textit{String Wrappers} were used in this tool to cached additional information about strings as needed. But in the real case database entities have non-trivial structure where most of record linkages perform on entities represent as vectors of match features. According to the authors the best method they propose is a scaled version of \textbf{Levenstein edit-distance} metric, modified by a method proposed by \textbf{WInkler} for the \textbf{Jaro distance}metric, with the scores for corresponding fields being adaptively combined by a (SVM) binary classifier.

\section*{A Comparison of Fast Blocking Methods for Record Linkage}
\cite{Baxter2003ALinkage} Blocking methods are used to reduce the number of candidate record linkage pairs in record linkage process while maintaining the linkage accuracy.Computational complexity increases with the comparison of records in large datasets where the possible record comparisons grow quadratically.To reduce the number of record comparisons traditional \textit{blocking} methods could be used which only allows to compare the records within the same block. Choice of a good blocking method greatly reduce the number of comparison and improve the performance of the record linkage process. The work of this paper discussed on comparing the speed and accuracy of new blocking methods with the existing blocking method implementations. The compared methods are \textit{Standard Blocking}[],\textbf{Sorted Neighborhood Method}[], \textit{Bigram indexing}[] and \textit{Canopy Clustering with TFIDF}[]. Linkage accuracy is crucial by avoid adding unnecessary noise in this research since the authors have used public health records for the linkage process.

Clustering the records using a identical blocking key is the method use in \textit{Standard Blocking}. Using the least error prone attributes for the blocking key and use of phonetic encoding to reduce the effect of transcription errors are some strategies to improve the linkage quality. In \textit{Sorted Neighborhood} method, it sorts the record according to \textit{sort key} and moves a window of fixed size 'w' over sorted records. Records falls under the window will be compared. Blocking key values are converted to a list of bigrams in \textit{Bigram Indexing}  and sub-list of possible permutations will be built using a threshold. The sublist will then be sorted and into an inverted index, which will be used to recieve the corresponding record numbers in a block. \textit{Canopy clustering choose a record randomly from candidates and then putting in its cluster all the records within a chosen \textit{loose threshold distance}}. The records chosen at a random and any records within a certain \textit{tight threshold} distance of it are then removed from the candidate set of records. They have used TFIDF distance metrics, where bigrams are used as tokens.

Reduction Ratio, Pairs Completeness and F Score has been used as evaluation matrices where the indexing methods were performed over artificial data sets from a widely used database generator. Results shows that the both Bigram Indexing and Canopy Clustering methods out perform the standard blocking method and Sorted Neighborhood, with the right parameter settings with a significant improved performance. 



\bibliography{Mendeley}
\bibliographystyle{ieeetr}


 \end{document}

